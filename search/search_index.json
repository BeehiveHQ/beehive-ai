{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Beehive","text":"<p>Beehive is an open-source framework for building AI agents and enabling these agents to cooperate with one another to solve tasks. This project was heavily inspired by the awesome work at Langgraph, CrewAI, and PyAutogen.</p> <p>We're still in the early stages, so any and all feedback is welcome! If you notice a bug or want to suggest an improvement, please open a Github PR.</p>"},{"location":"#changelog","title":"Changelog","text":"<p>N/A \u2014\u00a0this is Beehive's first release!</p>"},{"location":"#why-use-beehive","title":"Why use Beehive?","text":"<p>In traditional software applications, the chain of actions taken by the application in response to user input is hardcoded. Any \"reasoning\" that these applications employ (e.g., if the user does \"X\", do \"Y\", otherwise do \"Z\") can be traced to a few lines of code.</p> <p>On the other hand, agents rely on a language model to decide which actions to take and in what order. Unlike traditional software applications, where the sequence of actions is predefined in the code, the language model itself is the decision-making engine.</p> <p>Beehive, in particular, is great for rapidly creating complex chat patterns between agents (or invokables, in Beehive nomenclature). This includes:</p> <ul> <li>Sequential chats</li> <li>Hierarchical chats</li> <li>Multi-agent collaboration / debates</li> <li>Nested patterns</li> </ul> <p>In addition, Beehive shares many features with other popular agentic frameworks:</p> <ul> <li>Role-based agent design</li> <li>Loops and conditionals between agents</li> <li>State management</li> <li>Streaming support</li> <li>Memory / feedback</li> </ul> <p>Here is an example of something you could create in Beehive with relatively little code:</p> <p></p> <p>To learn how, head over to Getting Started to install Beehive.</p>"},{"location":"getting_started/","title":"Getting started","text":""},{"location":"getting_started/#installation","title":"Installation","text":"<p>You can install Beehive with <code>pip</code>: <pre><code>pip install beehive\n</code></pre></p> <p>Note that the standard Beehive installation includes the OpenAI client and a few Langchain packages.</p>"},{"location":"getting_started/#setting-up-your-first-beehive","title":"Setting up your first Beehive","text":"<p>Here's a simple Beehive that instructions two agents to work together to create a new language:</p> <pre><code>linguist_agent = BeehiveAgent(\n    name=\"Linguist\",\n    backstory=\"You are an expert in linguistics. You work alongside another linguist to develop new languages.\"\n    model=OpenAIModel(\n        model=\"gpt-4-turbo\",\n    ),\n)\n\nlinguist_critic = BeehiveAgent(\n    name=\"LinguistCritic\",\n    backstory=\"You are an expert in linguistics. Specifically, you are great at examining grammatical rules of new languages and suggesting improvements.\",\n    model=OpenAIModel(\n        model=\"gpt-4-turbo\",\n    ),\n)\n\nbeehive = Beehive(\n    name=\"LanguageGeneratorBeehive\",\n    backstory=\"You are an expert in creating new languages.\",\n    model=OpenAIModel(\n        model=\"gpt-4-turbo\",\n    ),\n    execution_process=FixedExecution(route=(linguist_agent &gt;&gt; linguist_critic)),\n    chat_loop=2,\n    enable_questioning=True,\n)\nbeehive.invoke(\n    \"Develop a new language using shapes and symbols. After you have developed a comprehensive set of grammar rules, provide some examples of sentences and their representation in the new language.\",\n    pass_back_model_errors=True\n)\n</code></pre> <p>Here is the output of that invokation:</p> LanguageGeneratorBeehive <code>stdout</code> <pre><code>------------------------------------------------------------------------------------------------------------------------\nLanguageGeneratorBeehive / Linguist\n\nAs an expert in linguistics with a focus on developing new languages, create a language using shapes and symbols. Define\na comprehensive set of grammar rules for this language. Then, provide examples of sentences in English alongside their\nrepresentations in the newly created symbolic language.\n\nCreating a language based on shapes and symbols involves defining a set of visual elements that can represent phonetic\nsounds, grammatical structures, and semantic meanings. For this exercise, let's create a basic symbolic language called\n\"Forma.\"\n\n### Basic Elements of Forma\n1. **Shapes**: Circles, triangles, squares, and lines.\n2. **Colors**: Red, blue, green, and black.\n3. **Size**: Large and small.\n4. **Position**: Overlapping, adjacent, inside.\n\n### Phonetics\n- **Circles** represent vowels.\n- **Triangles** represent consonants.\n- **Squares** represent modifiers (like tense or plurality).\n- **Lines** are connectors or relational symbols (like prepositions).\n\n### Colors and Sizes\n- **Red**: Past tense or negative.\n- **Blue**: Present tense or question.\n- **Green**: Future tense or emphasis.\n- **Black**: Neutral or statement.\n- **Large shapes**: Emphasis or formal tone.\n- **Small shapes**: Common speech or informal tone.\n\n### Positional Grammar\n- **Overlapping shapes**: Compound words or linked ideas.\n- **Adjacent shapes**: Separate words or ideas.\n- **Shapes inside others**: Dependent clauses or possession.\n\n### Example Grammar Rules\n1. **Sentence Structure**: Subject (triangle) + Verb (circle) + Object (triangle) + Modifier (square). Modifiers follow\nthe word they modify.\n2. **Questions**: Blue circle at the start of the sentence.\n3. **Negation**: A small red triangle before the verb.\n4. **Tense**: Color of the square after the verb indicates tense (red for past, blue for present, green for future).\n\n### Example Sentences\n1. **English**: \"I eat apples.\"\n- **Forma**: \ud83d\udd3a (I) \ud83d\udd35\u26aa (eat, present) \ud83d\udd3a (apples)\n\n2. **English**: \"Did you see the dog?\"\n- **Forma**: \ud83d\udd35\u26aa (question) \ud83d\udd3a (you) \ud83d\udd35\u26aa (see, past) \ud83d\udd3a (the dog)\n\n3. **English**: \"She will not go to the park.\"\n- **Forma**: \ud83d\udd3a (she) \ud83d\udfe2\u26aa (will go, future) \ud83d\udd34\ud83d\udd3a (not) \u2796 \ud83d\udd3a (to) \ud83d\udd3a (the park)\n\n4. **English**: \"They are reading a book.\"\n- **Forma**: \ud83d\udd3a (they) \ud83d\udd35\u26aa (are reading, present) \ud83d\udd3a (a book)\n\n5. **English**: \"Can I have some water?\"\n- **Forma**: \ud83d\udd35\u26aa (question) \ud83d\udd3a (I) \ud83d\udd35\u26aa (have, present) \ud83d\udd3a (some water)\n\n### Complex Sentence\n- **English**: \"The quick brown fox jumps over the lazy dog.\"\n- **Forma**: \ud83d\udd3a (The quick brown fox) \ud83d\udd35\u26aa (jumps, present) \u2796 \ud83d\udd3a (over) \ud83d\udd3a (the lazy dog)\n\nThis symbolic language, Forma, uses visual elements to convey meaning, which can be expanded with more complex rules and\nadditional symbols for more nuanced expressions. The examples provided are basic and intended to illustrate the\nfoundational structure of the language.\n------------------------------------------------------------------------------------------------------------------------\nLanguageGeneratorBeehive / Router\n\nSending conversation to LinguistCritic next!\n------------------------------------------------------------------------------------------------------------------------\nLanguageGeneratorBeehive / LinguistCritic\n\nBased on the foundational structure of the symbolic language 'Forma' previously outlined, please refine and expand the\ngrammar rules to include more complex sentence structures and additional modifiers. Also, develop a more nuanced system\nfor expressing emotions and interrogative forms using the existing shapes and colors. Provide updated examples of\ncomplex sentences and their representations in 'Forma'.\n\nTo refine and expand the grammar rules of the symbolic language 'Forma', and to include more complex sentence structures\nand additional modifiers, as well as a nuanced system for expressing emotions and interrogative forms, we can proceed as\nfollows:\n\n### Expanded Grammar Rules for Complex Sentence Structures\n\n1. **Compound Sentences**: Use a double line (\u23af\u23af) to connect two independent clauses. Each clause follows the basic\nsentence structure but is considered part of a larger sentence.\n\n2. **Subordinate Clauses**: Introduce a smaller square inside the main square to indicate a subordinate clause, which\nprovides additional information about the main clause.\n\n3. **Passive Voice**: Use a reversed triangle (\u25bd) to indicate the passive voice. The object of the action comes before\nthe verb, followed by the reversed triangle and then the subject.\n\n### Additional Modifiers\n\n1. **Adverbs**: Introduce hexagons to represent adverbs. Place the hexagon adjacent to the verb it modifies. The color\nindicates the type of adverb:\n- **Red**: Negative adverb.\n- **Blue**: Manner adverb.\n- **Green**: Time adverb.\n\n2. **Comparatives and Superlatives**: Use stacked triangles for comparatives (two triangles) and superlatives (three\ntriangles). The size of the triangles indicates the degree of comparison.\n\n### Expressing Emotions\n\n1. **Emotional Tone**: Introduce a border around the sentence. The style of the border indicates the emotion:\n- **Dotted line**: Uncertainty or confusion.\n- **Wavy line**: Excitement or happiness.\n- **Jagged line**: Anger or frustration.\n\n### Nuanced Interrogative Forms\n\n1. **Yes/No Questions**: A blue circle at the start of the sentence.\n2. **Wh-Questions**: A blue square at the start of the sentence, with small symbols inside to indicate the type of\nquestion (who, what, where, etc.).\n\n### Updated Example Sentences\n\n1. **English**: \"The cat, which was hungry, ate the food.\"\n- **Forma**: \ud83d\udd3a (The cat) \ud83d\udd32\ud83d\udd3a (which was hungry) \ud83d\udd35\u26aa (ate, past) \ud83d\udd3a (the food)\n\n2. **English**: \"Did you quickly run to the store?\"\n- **Forma**: \ud83d\udd35\u26aa (question) \ud83d\udd3a (you) \ud83d\udd35\u2b21 (quickly) \ud83d\udd35\u26aa (run, past) \u2796 \ud83d\udd3a (to the store)\n\n3. **English**: \"She has not been seen by anyone.\"\n- **Forma**: \ud83d\udd3a (She) \ud83d\udd34\u2b21 (not) \ud83d\udd35\u26aa (been seen, past) \u25bd \ud83d\udd3a (by anyone)\n\n4. **English**: \"I am very happy to meet you.\"\n- **Forma**: \ud83d\udd3a (I) \ud83d\udd35\u26aa (am, present) \ud83d\udd35\ud83d\udd3a\ud83d\udd3a (very) \ud83c\udf0a (happy) \u2796 \ud83d\udd3a (to meet) \ud83d\udd3a (you)\n\nThese expansions and refinements in the grammar and symbolic representations of 'Forma' allow for more detailed and\nexpressive communication, accommodating complex sentence structures, additional modifiers, emotional expressions, and\nnuanced interrogative forms.\n------------------------------------------------------------------------------------------------------------------------\nLanguageGeneratorBeehive / Linguist\n\nRefine and expand the grammar rules of the symbolic language 'Forma' to include more complex sentence structures and\nadditional modifiers. Develop a nuanced system for expressing emotions and interrogative forms using the existing shapes\nand colors. Provide updated examples of complex sentences and their representations in 'Forma'. Ensure that the\nmodifications are coherent with the foundational structure previously established.\n\nTo refine and expand the grammar rules of the symbolic language 'Forma', and to include more complex sentence structures\nand additional modifiers, as well as a nuanced system for expressing emotions and interrogative forms, we can proceed as\nfollows:\n\n### Expanded Grammar Rules for Complex Sentence Structures\n\n1. **Compound Sentences**: Use a double line (\u23af\u23af) to connect two independent clauses. Each clause follows the basic\nsentence structure but is considered part of a larger sentence.\n\n2. **Subordinate Clauses**: Introduce a smaller square inside the main square to indicate a subordinate clause, which\nprovides additional information about the main clause.\n\n3. **Passive Voice**: Use a reversed triangle (\u25bd) to indicate the passive voice. The object of the action comes before\nthe verb, followed by the reversed triangle and then the subject.\n\n### Additional Modifiers\n\n1. **Adverbs**: Introduce hexagons to represent adverbs. Place the hexagon adjacent to the verb it modifies. The color\nindicates the type of adverb:\n- **Red**: Negative adverb.\n- **Blue**: Manner adverb.\n- **Green**: Time adverb.\n\n2. **Comparatives and Superlatives**: Use stacked triangles for comparatives (two triangles) and superlatives (three\ntriangles). The size of the triangles indicates the degree of comparison.\n\n### Expressing Emotions\n\n1. **Emotional Tone**: Introduce a border around the sentence. The style of the border indicates the emotion:\n- **Dotted line**: Uncertainty or confusion.\n- **Wavy line**: Excitement or happiness.\n- **Jagged line**: Anger or frustration.\n\n### Nuanced Interrogative Forms\n\n1. **Yes/No Questions**: A blue circle at the start of the sentence.\n2. **Wh-Questions**: A blue square at the start of the sentence, with small symbols inside to indicate the type of\nquestion (who, what, where, etc.).\n\n### Updated Example Sentences\n\n1. **English**: \"The cat, which was hungry, ate the food.\"\n- **Forma**: \ud83d\udd3a (The cat) \ud83d\udd32\ud83d\udd3a (which was hungry) \ud83d\udd34\u26aa (ate, past) \ud83d\udd3a (the food)\n\n2. **English**: \"Did you quickly run to the store?\"\n- **Forma**: \ud83d\udd35\u26aa (question) \ud83d\udd3a (you) \ud83d\udd35\u2b21 (quickly) \ud83d\udd35\u26aa (run, past) \u2796 \ud83d\udd3a (to the store)\n\n3. **English**: \"She has not been seen by anyone.\"\n- **Forma**: \ud83d\udd3a (She) \ud83d\udd34\u2b21 (not) \ud83d\udd34\u26aa (been seen, past) \u25bd \ud83d\udd3a (by anyone)\n\n4. **English**: \"I am very happy to meet you.\"\n- **Forma**: \ud83d\udd3a (I) \ud83d\udd35\u26aa (am, present) \ud83d\udd35\ud83d\udd3a\ud83d\udd3a (very) \ud83c\udf0a (happy) \u2796 \ud83d\udd3a (to meet) \ud83d\udd3a (you)\n\nThese expansions and refinements in the grammar and symbolic representations of 'Forma' allow for more detailed and\nexpressive communication, accommodating complex sentence structures, additional modifiers, emotional expressions, and\nnuanced interrogative forms.\n------------------------------------------------------------------------------------------------------------------------\nLanguageGeneratorBeehive / Router\n\nSending conversation to LinguistCritic next!\n------------------------------------------------------------------------------------------------------------------------\nLanguageGeneratorBeehive / LinguistCritic\n\nBased on the foundational structure and recent expansions of the symbolic language 'Forma', further refine the grammar\nrules to include additional complex sentence structures and modifiers. Develop a more detailed system for expressing\nemotions and interrogative forms using the existing shapes and colors. Provide updated examples of complex sentences and\ntheir representations in 'Forma'. Ensure that the modifications are coherent with the foundational structure previously\nestablished and align with the principles of linguistic development.\n\nTo further refine the grammar rules of the symbolic language 'Forma' and develop a more detailed system for expressing\nemotions and interrogative forms, we can introduce the following enhancements:\n\n### Additional Complex Sentence Structures\n\n1. **Conditional Sentences**: Use a dashed line ( - - - ) to connect the conditional clause to the main clause. The\nconditional clause starts with a pentagon to indicate the conditional mood.\n\n2. **Causative Constructions**: Introduce a small circle within a triangle to indicate that the action is caused by the\nsubject but performed by another. This structure emphasizes the initiator of the action rather than the performer.\n\n3. **Relative Clauses**: Use a semi-transparent overlay on the shape representing the noun that the relative clause\nmodifies. This visually connects the clause to the noun.\n\n### Enhanced Modifiers\n\n1. **Frequency Adverbs**: Introduce ellipses to represent frequency (often, sometimes, rarely). The number of dots\nwithin the ellipse indicates the frequency (more dots for higher frequency).\n\n2. **Degree Modifiers**: Use concentric shapes to indicate the degree of intensity (very, quite, slightly). The number\nof concentric layers corresponds to the degree of intensity.\n\n### Detailed System for Expressing Emotions\n\n1. **Specific Emotions**: Introduce specific patterns within the borders to represent different emotions:\n- **Stripes**: Confusion or mixed feelings.\n- **Spirals**: Surprise or shock.\n- **Stars**: Joy or excitement.\n\n2. **Intensity of Emotions**: The thickness of the border indicates the intensity of the emotion. Thicker borders\nrepresent stronger emotions.\n\n### Nuanced Interrogative Forms\n\n1. **Option Questions**: Use a series of small blue triangles to indicate choices in a question (e.g., \"Would you like\ntea or coffee?\").\n\n2. **Explanatory Questions**: Introduce a blue hexagon at the start of the sentence to indicate a request for\nexplanation (e.g., \"Why is this happening?\").\n\n### Updated Example Sentences\n\n1. **English**: \"If it rains, the picnic will be canceled.\"\n- **Forma**: \u2b1f (if) \ud83d\udd35\u26aa (rains, present) - - - \ud83d\udd3a (the picnic) \ud83d\udd35\ud83d\udd32 (will be, future) \ud83d\udd35\u26aa (canceled, passive)\n\n2. **English**: \"She makes him do his homework.\"\n- **Forma**: \ud83d\udd3a (She) \ud83d\udd35\u26aa (makes, present) \ud83d\udd3a\u26aa (him) \ud83d\udd35\u26aa (do, present) \ud83d\udd3a (his homework)\n\n3. **English**: \"The book that I read was interesting.\"\n- **Forma**: \ud83d\udd3a (The book) \u2b1b\ud83d\udd3a (that I read) \ud83d\udd35\u26aa (was, past) \ud83d\udd35\ud83c\udf1f (interesting)\n\n4. **English**: \"How often do you go to the gym?\"\n- **Forma**: \ud83d\udd35\u2b21 (how often) \ud83d\udd35\u26aa (do, present) \ud83d\udd3a (you) \ud83d\udd35\u26aa (go, present) \ud83d\udd3a (to the gym)\n\nThese further refinements and expansions in the grammar and symbolic representations of 'Forma' enhance the language's\nability to convey complex ideas, nuanced emotions, and detailed interrogative forms, maintaining coherence with the\nfoundational structure and principles of linguistic development.\n------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>In this Beehive, we have two invokables that work together to create a new language. The <code>Linguist</code> is responsible for creating the initial grammatical rules, and the <code>LinguistCritic</code> is responsible for suggesting improvements to those rules. The Beehive is only able to follow a strict path: the <code>Linguist</code> is invoked first, followed by the <code>LinguistCritic</code>. Internally, we enable these agents to collaborate and build off of another other by passing each other their <code>state</code> prior to the other's invokation. Moreover, we wrap our execution in a loop of length 2 (via <code>chat_loop</code>) in order to force these invokables to have a lengthier conversation.</p> <p></p> <p>We'll cover each of these concepts next.</p>"},{"location":"core_concepts/beehives/","title":"Beehives","text":"<p>Beehives enable different invokables to collaborate with one another to achieve a task.</p>"},{"location":"core_concepts/beehives/#attributes","title":"Attributes","text":"Attribute<code>type</code> Description name<code>str</code> The invokable name. backstory<code>str</code> Backstory for the AI actor. This is used to prompt the AI actor and direct tasks towards it. Default is: 'You are a helpful AI assistant.' model<code>BHChatModel | BaseChatModel</code> Chat model used by the invokable to execute its function. This can be a <code>BHChatModel</code> or a Langchain <code>ChatModel</code>. chat_loop<code>int</code> Number of times the model should loop when responding to a task. Usually, this will be 1, but certain prompting patterns may require more loops (e.g., chain-of-thought prompting). state<code>list[BHMessage | BHToolMessage] | list[BaseMessage]</code> List of messages that this actor has seen. This enables the actor to build off of previous conversations / outputs. execution_process<code>FixedExecution | DynamicExecution</code> Execution process, either <code>FixedExecution</code> or <code>DynamicExecution</code>. If <code>FixedExecution</code>, then the Beehive will execute the Invokables in the <code>FixedExecution.route</code>. If <code>DynamicExecution</code>, then Beehive uses an internal router agent to determine which <code>Invokable</code> to act given the previous messages / conversation. enable_questioning<code>bool</code> Enable invokables to ask one another clarifying questions at runtime. history<code>bool</code> Whether to use previous interactions / messages when responding to the current task. Default is <code>False</code>. history_lookback<code>int</code> Number of days worth of previous messages to use for answering the current task. feedback<code>bool</code> Whether to use feedback from the invokable's previous interactions. Feedback enables the LLM to improve their responses over time. Note that only feedback from tasks with a similar embedding are used. feedback_embedder<code>BHEmbeddingModel | None</code> Embedding model used to calculate embeddings of tasks. These embeddings are stored in a vector database. When a user prompts the Invokable, the Invokable searches against this vector database using the task embedding. It then takes the suggestions generated for similar, previous tasks and concatenates them to the task prompt. Default is <code>None</code>. feedback_prompt_template<code>str | None</code> Prompt for evaluating the output of an LLM. Used to generate suggestions that can be used to improve the model's output in the future. If <code>None</code>, then Beehive will use the default prompt template. Default is <code>None</code>. feedback_model<code>BHChatModel | BaseChatModel</code> Language model used to generate feedback for the invokable. If <code>None</code>, then default to the <code>model</code> attribute. feedback_embedding_distance<code>EmbeddingDistance</code> Distance method of the embedding space. See the ChromaDB documentation for more information: https://docs.trychroma.com/guides#changing-the-distance-function. n_feedback_results<code>int</code> Amount of feedback to incorporate into answering the current task. This takes <code>n</code> tasks with the most similar embedding to the current one and incorporates their feedback into the Invokable's model. Default is <code>1</code>. color<code>str</code> Color used to represent the invokable in verbose printing. This can be a HEX code, an RGB code, or a standard color supported by the Rich API. See https://rich.readthedocs.io/en/stable/appendix/colors.html for more details. Default is <code>chartreuse2</code>. <p>Note</p> <p>Unlike <code>BeehiveAgents</code>, <code>BeehiveLangchainAgents</code>, <code>BeehiveEnsembles</code>, and <code>BeehiveDebateTeams</code> the <code>Beehive</code> class does NOT accept any tools. This is because the <code>Beehive</code> instance doesn't actually do the work of executing the task. Rather, it enables the invokables specified in its execution process to execute the task by collaborating together. As such, the <code>Beehive</code> will not use tools, but the worker invokables may (and often should) be able to.</p> <p>Notice that a <code>Beehive</code> is actually a subclass of the <code>Invokable</code> base class. In fact, most of the fields are those we've already seen!</p> <p>The two additions are the <code>execution_process</code> and <code>enable_questioning</code> fields. These controls how the conversation flows between the invokables. We'll cover these next.</p>"},{"location":"core_concepts/beehives/#execution-process","title":"Execution process","text":"<p>Beehive supports both fixed routing and dynamic routing:</p> <ul> <li>Fixed routing: the Beehive invokes <code>Invokables</code> according to a pre-defined order</li> <li>Dynamic routing: the Beehive uses an internal router agent to determine which <code>Invokable</code> to act given the previous messages / conversation.</li> </ul> <p>This routing is controlled by the <code>execution_process</code> keyword argument, and the value can be either an instance of the <code>FixedExecution</code> class or the <code>DynamicExecution</code> class:</p> <pre><code>class FixedExecution(BaseModel):\n    route: Route\n\n\nclass DynamicExecution(BaseModel):\n    edges: list[Route]\n    entrypoint: Invokable\n    llm_router_additional_instructions: dict[Invokable, list[str]]\n</code></pre> <p>The <code>Route</code> type represents a path between any two <code>Invokables</code>. It is specified via the <code>&gt;&gt;</code> bitwise operator.</p>"},{"location":"core_concepts/beehives/#fixedexecution","title":"FixedExecution","text":"<p>The <code>FixedExecution</code> class accepts a single route \u2014 this is the route that the Beehive will use when sending the conversation from one invokable to the next.</p> <pre><code>from beehive.invokable.agent import BeehiveAgent\nfrom beehive.invokable.beehive import Beehive, FixedExecution\nfrom beehive.models.openai_model import OpenAIModel\n\nbackstory_template = \"{backstory} You are collaborating with other assistants powered by language models. Answer questions to the best of your ability. It is acceptable if you cannot fully complete the task, as other agents will build on your work. Your goal is to provide valuable input that moves the task forward.\"\n\nmath_agent = BeehiveAgent(\n    name=\"MathAgent\",\n    backstory=backstory_template.format(backstory=\"You are a helpful AI assistant that specializes in performing complex calculations.\"),\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo\",\n        api_key=\"&lt;your_api_key&gt;\"\n    )\n)\n\nmath_qc_agent = BeehiveAgent(\n    name=\"MathQCAgent\",\n    backstory=backstory_template.format(backstory=\"You are math critic who specializes in identifying errors in calculations.\"),\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n)\n\nworkflow = Beehive(\n    name=\"CalculationBeehive\",\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    execution_process=FixedExecution(\n        route=(math_agent &gt;&gt; math_qc_agent)\n    )\n)\noutput = workflow.invoke(\n    task=\"A treasure hunter found a buried treasure chest filled with gems. There were 175 diamonds, 35 fewer rubies than diamonds, and twice the number of emeralds than the rubies. How many of the gems were there in the chest?\",\n)\n</code></pre> <p>As you can see, in our <code>FixedExecution</code> configuration, we specified a route that starts at <code>math_agent</code> and ends at <code>math_qc_agent</code>. Here's what happens under the hood:</p> <ul> <li>Beehive starts by prompting <code>MathAgent</code> because it is using a fixed execution route and <code>math_agent</code> is the first agent in this route. Beehive will use the user's original task when prompting the `Researcher agent.</li> <li>After <code>MathAgent</code> finishes, Beehive will use its output and craft a prompt for the next invokable in the route, namely <code>MathQCAgent</code>.</li> </ul> <p>When the Beehive executes, here is what is printed to <code>stdout</code>: <pre><code>------------------------------------------------------------------------------------------------------------------------\nCalculationBeehive / MathAgent\n\nCalculate the total number of gems in a treasure chest given the following: there are 175 diamonds, 35 fewer rubies than\ndiamonds, and twice the number of emeralds as rubies. Provide the calculation steps and the final total.\n\nTo find the total number of gems in the treasure chest, we need to calculate the number of each type of gem and then sum\nthem up.\n\n1. **Number of Diamonds**: Given as 175.\n\n2. **Number of Rubies**: There are 35 fewer rubies than diamonds.\n   \\[\n   \\text{Number of Rubies} = 175 - 35 = 140\n   \\]\n\n3. **Number of Emeralds**: There are twice the number of emeralds as rubies.\n   \\[\n   \\text{Number of Emeralds} = 2 \\times 140 = 280\n   \\]\n\nNow, add all these together to find the total number of gems:\n\\[\n\\text{Total Number of Gems} = \\text{Number of Diamonds} + \\text{Number of Rubies} + \\text{Number of Emeralds} = 175 +\n140 + 280\n\\]\n\\[\n\\text{Total Number of Gems} = 595\n\\]\n\nThus, the treasure chest contains a total of 595 gems.\n------------------------------------------------------------------------------------------------------------------------\nCalculationBeehive / Router\n\nSending conversation to MathQCAgent next!\n------------------------------------------------------------------------------------------------------------------------\nCalculationBeehive / MathQCAgent\n\nPlease review the calculation provided by the previous agent for any potential errors or improvements. The calculation\ninvolves the total number of gems in a treasure chest, with specific counts for diamonds, rubies, and emeralds based on\ngiven conditions. Confirm the accuracy of the arithmetic operations and the logical application of the conditions\ndescribed.\n\nLet's review the calculations provided by the previous agent, MathAgent, for accuracy and logical consistency:\n\n1. **Number of Diamonds**: Given as 175.\n   - This is straightforward and requires no calculation.\n\n2. **Number of Rubies**: Calculated as 35 fewer than the number of diamonds.\n   - Calculation: \\(175 - 35 = 140\\)\n   - This is correctly calculated based on the information provided.\n\n3. **Number of Emeralds**: Calculated as twice the number of rubies.\n   - Calculation: \\(2 \\times 140 = 280\\)\n   - This is also correctly calculated based on the information provided.\n\n4. **Total Number of Gems**: Sum of diamonds, rubies, and emeralds.\n   - Calculation: \\(175 + 140 + 280\\)\n   - Adding these numbers gives \\(595\\), which matches the total provided by the previous agent.\n\nBased on the review, the arithmetic operations are correctly executed, and the logical application of the conditions (35\nfewer rubies than diamonds and twice as many emeralds as rubies) is accurately applied. The total number of gems\ncalculated as 595 appears to be correct.\n\nNo errors or improvements are needed for the calculations provided. The previous agent has accurately computed the total\nnumber of gems in the treasure chest based on the conditions and numbers given.\n------------------------------------------------------------------------------------------------------------------------\n</code></pre></p>"},{"location":"core_concepts/beehives/#dynamicexecution","title":"DynamicExecution","text":"<p>The <code>DynamicExecution</code> class accepts three arguments:</p> <ul> <li><code>edges</code>: a list of Routes (again, specified via the <code>&gt;&gt;</code> bitwise operator). These edges tell the Beehive which Invokables can speak to one another.</li> <li><code>entrypoint</code>: the Invokable that the Beehive should start at.</li> <li><code>llm_router_additional_instructions</code>: a mapping of Invokables to additional instructions to help Beehive's internal routing agent determine the best next actor.</li> </ul> <pre><code>from beehive.invokable.agent import BeehiveAgent\nfrom beehive.invokable.beehive import Beehive, DynamicExecution\nfrom beehive.models.openai_model import OpenAIModel\n\nbackstory_template = \"{backstory} You are collaborating with other assistants powered by language models. Answer questions to the best of your ability. It is acceptable if you cannot fully complete the task, as other agents will build on your work. Your goal is to provide valuable input that moves the task forward.\"\n\nresearcher = BeehiveAgent(\n    name=\"Researcher\",\n    backstory=backstory_template.format(backstory=\"You are a researcher that has access to the web via your tools.\"),\n    model=OpenAIModel(\n        model=\"gpt-4-turbo\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    tools=[tavily_search_tool],  # tool for surfing the web\n    history=True,\n    feedback=True,\n)\n\nchart_generator = BeehiveAgent(\n    name=\"ChartGenerator\",\n    backstory=backstory_template.format(backstory=\"You are a data analyst that specializes in writing and executing code to produce visualizations.\"),\n    model=OpenAIModel(\n        model=\"gpt-4-turbo\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    tools=[python_repl],  # tool for executing Python code\n    history=True,\n    feedback=True,\n)\nchart_generator_qc = BeehiveAgent(\n    name=\"ChartGeneratorCritic\",\n    backstory=backstory_template.format(backstory=\"You are an expert QA engineer who specializes in identifying errors in Python code.\"),\n    model=OpenAIModel(\n        model=\"gpt-4-turbo\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    history=True,\n    feedback=True,\n)\n\nworkflow = Beehive(\n    name=\"CalculationBeehive\",\n    model=OpenAIModel(\n        model=\"gpt-4-turbo\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    execution_process=DynamicExecution(\n        edges=[\n            researcher &gt;&gt; chart_generator,\n            chart_generator &gt;&gt; chart_generator_qc,\n            chart_generator_qc &gt;&gt; researcher,\n        ],\n        entrypoint=researcher,\n        llm_router_additional_instructions={\n            chart_generator_qc: [\n                \"If you notice a `ModuleNotFoundError` in the code, then finish.\n            ]\n        },\n    )\n)\noutput = workflow.invoke(\n    \"Fetch the UK's GDP over the past 5 years a draw a line graph of the data.\"\n)\n</code></pre> <p>Note</p> <p>The <code>tavily_search_tool</code> used by the <code>Researcher</code> agent is simply a function. Its implementation can be found here.</p> <p>In this Beehive, we have three agents: a <code>Researcher</code> agent that browses the internet, a <code>ChartGenerator</code> agent that generates Python code, and a <code>ChartGeneratorCritic</code> agent that QAs Python code. Here's what happens under the hood:</p> <ul> <li>Since the <code>entrypoint</code> is set to the <code>Researcher</code> agent, that's where Beehive will start. Beehive will use the user's original task when prompting the `Researcher agent.</li> <li>After the <code>Researcher</code> finishes, Beehive first determines if additional action is needed. If it is, then it looks at the <code>edges</code> and determines which other invokables the <code>Researcher</code> is allowed to talk to. In this case, <code>Researcher</code> is only allowed to speak to the <code>ChartGenerator</code> agent, so Beehive's internal router crafts a prompt for the <code>ChartGenerator</code> agent.</li> <li>Similarly, after the <code>ChartGenerator</code> finishes, Beehive once again determines if additional action is needed. If it does, it looks at the user-specified <code>edges</code> and finds that the output of <code>ChartGenerator</code> can only be piped to the <code>ChartGeneratorQC</code> agent. Assuming Beehive's internal router determines that action is required from the <code>ChartGeneratorQC</code> agent, it crafts a new prompt for the agent.</li> <li>After the <code>ChartGeneratorQC</code> agent finishes, Beehive determines if additional action is needed. Note here that, when making this determination, Beehive uses the <code>llm_router_additional_instructions</code> that the user specified for <code>ChartGeneratorQC</code> \u2014 namely, that if this agent noticed a <code>ModuleNotFoundError</code> in the code that the <code>ChartGenerator</code> produced, then it should finish. Based on this determination, the Beehive will either finish or will send the conversation back to the <code>Researcher</code> agent.</li> </ul> <p>When the Beehive executes, here is what is printed to <code>stdout</code>: <pre><code>------------------------------------------------------------------------------------------------------------------------\nCalculationBeehive / Researcher\n\nPlease search the web to find the most recent data on the UK's GDP for the past five years. Compile this data into a\nlist, including the GDP values and corresponding years. Then, pass this information to the next agent who will create a\nline graph based on the data provided.\n\n The UK GDP for the past five years is as follows:\n- 2023: GDP was not provided in the data.\n- 2022: GDP data was not available in the provided sources.\n- 2021: GDP was $3,141.51 billion with a growth rate of 8.67%.\n- 2020: GDP was $2,697.81 billion with a growth rate of -10.36%.\n- 2019: GDP was $2,851.41 billion with a growth rate of 1.64%.\n\nPlease note that the most recent data available is for 2021.\n------------------------------------------------------------------------------------------------------------------------\nCalculationBeehive / Router\n\nSending conversation to ChartGenerator next!\n------------------------------------------------------------------------------------------------------------------------\nCalculationBeehive / ChartGenerator\n\nCreate a line graph of the UK's GDP for the years 2019, 2020, and 2021 using the provided data: 2019: $2,851.41 billion,\n2020: $2,697.81 billion, 2021: $3,141.51 billion.\n\n[10/06/24 11:44:22] WARNING  Python REPL can execute arbitrary code. Use with caution.                                                                              python.py:17\n Successfully executed:\n```python\nimport matplotlib.pyplot as plt\n\n# Data for the UK's GDP for the years 2019, 2020, and 2021\nyears = [2019, 2020, 2021]\ngdp_values = [2851.41, 2697.81, 3141.51]\n\n# Creating the line graph\nplt.figure(figsize=(10, 5))\nplt.plot(years, gdp_values, marker='o')\nplt.title('UK GDP from 2019 to 2021')\nplt.xlabel('Year')\nplt.ylabel('GDP in Billion USD')\nplt.grid(True)\nplt.xticks(years)\nplt.show()\n</code></pre> Stdout: ModuleNotFoundError(\"No module named 'matplotlib'\")</p> <pre><code>## Questioning\n\nIn addition to specifying the execution process (fixed or dynamic), you can also enable invokables to directly ask one another questions via the `enable_questioning` keyword argument. When set to `True`, invokables can ask clarifying questions to any invokable that has acted before them (or any invokable that has a route leading to them, in the case of a `DynamicExecutionProcess`).\n\n!!! note\n    Allowing questions is almost certainly possible via the router through some clever prompt engineering. However, `enable_questioning` is an easier way to support this behavior!\n\nHere's an example where the field is enabled:\n\n```python\n...\nworkflow = Beehive(\n    name=\"CalculationBeehive\",\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    execution_process=DynamicExecution(\n        edges=[\n            researcher &gt;&gt; chart_generator,\n            chart_generator &gt;&gt; chart_generator_qc,\n            chart_generator_qc &gt;&gt; researcher,\n        ],\n        entrypoint=researcher,\n        llm_router_additional_instructions={\n            chart_generator_qc: [\n                \"If you notice a `ModuleNotFoundError` in the code, then finish.\n            ]\n        },\n    ),\n    enable_questioning=True,\n)\n</code></pre> <p>When the <code>ChartGenerator</code> acts after the <code>Researcher</code> agent, it can ask the <code>Researcher</code> agent clarifying questions about its output.</p> <p>Note</p> <p>Questioning (and in general, direction interaction between child invokables) is more representative of human conversation. We plan on extending invokable-to-invokable direct conversation in future versions of Beehive. If you have any ideas, please request a feature or open a PR!</p>"},{"location":"core_concepts/beehives/#increasing-complexity","title":"Increasing complexity","text":"<p>The examples above represent conversations between different <code>BeehiveAgents</code>. However, <code>Beehives</code> support conversations between any kind of <code>Invokable</code>, including other <code>Beehives</code>! This is where the real power of Beehive becomes apparent \u2014 using nested invokable types allows users to quickly spin up complex conversation structures. Let's look at some examples.</p>"},{"location":"core_concepts/beehives/#nested-beehive","title":"Nested Beehive","text":"<p>In the above chart generation example, we leave it up to our LLM router to determine whether or not to QC the code produced by <code>ChartGenerator</code>. What if we wanted to force this conversation between the <code>ChartGenerator</code> and <code>ChartGeneratorCritic</code> to take place prior to sending the conversation back to the <code>Researcher</code>? In that case, we could use a nested Beehive!</p> <p>Recall the edges / routes in a Beehive accept any class that inherits the <code>Invokable</code> base class. This includes other <code>Beehives</code>!</p> <p>Here's what that would look like:</p> <pre><code>from beehive.invokable.agent import BeehiveAgent\nfrom beehive.invokable.beehive import Beehive, DynamicExecution\nfrom beehive.models.openai_model import OpenAIModel\n\nresearcher = BeehiveAgent(\n    name=\"Researcher\",\n    backstory=\"You are a researcher that has access to the web via your tools.\"\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    tools=[tavily_search_tool],  # tool for surfing the web\n    history=True,\n    feedback=True,\n)\n\n# Separate Beehive for working creating a chart. This force the ChartGenerator to always\n# talk to the ChartGeneratorCritic after producing some code.\nchart_generator = BeehiveAgent(\n    name=\"ChartGenerator\",\n    backstory=\"You are a data analyst that specializes in writing and executing code to produce visualizations.\",\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    tools=[python_repl],  # tool for executing Python code\n    history=True,\n    feedback=True,\n)\nchart_generator_qc = BeehiveAgent(\n    name=\"ChartGeneratorCritic\",\n    backstory=\"You are an expert QA engineer who specializes in identifying errors in Python code.\",\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    history=True,\n    feedback=True,\n)\nchart_generator_workflow = Beehive(\n    name=\"ChartGeneratorWorkflow\",\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    execution_process=FixedExecution(route=(chart_generator &gt;&gt; chart_generator_qc)),\n    history=True,\n    feedback=True,\n)\n\nworkflow = Beehive(\n    name=\"CalculationBeehive\",\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    execution_process=DynamicExecution(\n        edges=[\n            researcher &gt;&gt; chart_generator_workflow,\n            chart_generator_workflow &gt;&gt; researcher,\n        ],\n        entrypoint=researcher,\n        llm_router_additional_instructions={\n            chart_generator_workflow: [\n                \"If you notice a `ModuleNotFoundError` in the code, then finish.\"\n            ]\n        },\n    )\n)\noutput = workflow.invoke(\n    \"Fetch the UK's GDP over the past 5 years a draw a line graph of the data.\"\n)\n</code></pre> <p>Note that our <code>chart_generator_workflow</code> is a <code>Beehive</code> with a <code>FixedExecution</code> process. This forces the conversation to pass from <code>ChartGenerator</code> to <code>ChartGeneratorCritic</code> whenever the Beehive is invoked.</p> <p>Here's what is printed to <code>stdout</code>:</p> Nested Beehive <pre><code>------------------------------------------------------------------------------------------------------------------------\nCalculationBeehive / Researcher\n\nFetch the UK's GDP over the past 5 years a draw a line graph of the data.\n\nThe UK GDP growth rate in the third quarter of 2023 was 0.30 percent year-on-year, with the economy expanding at a\nslower pace than initially estimated. The GDP Growth Rate in the United Kingdom is expected to be 0.20 percent by the\nend of the current quarter.\n------------------------------------------------------------------------------------------------------------------------\nCalculationBeehive / Router\n\nSending conversation to ChartGeneratorWorkflow next!\n------------------------------------------------------------------------------------------------------------------------\nCalculationBeehive / ChartGeneratorWorkflow / ChartGenerator\n\nGenerate a line graph of the UK's GDP over the past 5 years.\n\n[09/20/24 08:52:48] WARNING  Python REPL can execute arbitrary code. Use with caution.                                                                                    python.py:17\nSuccessfully executed:\n```python\nimport matplotlib.pyplot as plt\n\n# Data for the UK's GDP over the past 5 years\nyears = [2018, 2019, 2020, 2021, 2022]\ngdp_values = [2.855, 2.829, 2.743, 2.678, 2.715]  # in trillion USD\n\n# Create a line graph\nplt.figure(figsize=(10, 6))\nplt.plot(years, gdp_values, marker='o', color='b', linestyle='-', linewidth=2)\nplt.title('UK GDP Over the Past 5 Years')\nplt.xlabel('Year')\nplt.ylabel('GDP in Trillion USD')\nplt.grid(True)\nplt.show()\n```\nStdout: ModuleNotFoundError(\"No module named 'matplotlib'\")\n------------------------------------------------------------------------------------------------------------------------\nCalculationBeehive / ChartGeneratorWorkflow / Router\n\nSending conversation to ChartGeneratorCritic next!\n------------------------------------------------------------------------------------------------------------------------\nCalculationBeehive / ChartGeneratorWorkflow / ChartGeneratorCritic\n\nUpdate the Python code to include error handling for potential ModuleNotFoundError when importing matplotlib. If the\nimport fails, provide a message indicating that matplotlib is required for generating the graph. Ensure that the code\ngracefully handles this error scenario.\n\nTo handle the potential `ModuleNotFoundError` when importing `matplotlib`, you can use a `try-except` block to catch the\nerror and provide a custom message. Here's an updated version of the Python code with error handling for the\n`matplotlib` import:\n\n```python\ntry:\n    import matplotlib.pyplot as plt\nexcept ModuleNotFoundError:\n    print(\"Error: Matplotlib is required for generating the graph. Please install matplotlib to proceed.\")\nelse:\n    # Data for the UK's GDP over the past 5 years\n    years = [2018, 2019, 2020, 2021, 2022]\n    gdp_values = [2.855, 2.829, 2.743, 2.678, 2.715]  # in trillion USD\n\n    # Create a line graph\n    plt.figure(figsize=(10, 6))\n    plt.plot(years, gdp_values, marker='o', color='b', linestyle='-', linewidth=2)\n    plt.title('UK GDP Over the Past 5 Years')\n    plt.xlabel('Year')\n    plt.ylabel('GDP in Trillion USD')\n    plt.grid(True)\n    plt.show()\n```\n\nIn this updated code snippet:\n- We use a `try-except` block to attempt the import of `matplotlib.pyplot`. If the import fails due to a\n`ModuleNotFoundError`, a custom error message is printed.\n- If the import is successful, the code proceeds to create the line graph as before.\n\nThis error handling mechanism ensures that the user is informed about the missing `matplotlib` dependency and gracefully\nhandles the situation without causing the script to crash.\n------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>Note that the Beehive finishes after <code>ChartGeneratorCritic</code>! This is because of our <code>llm_additional_routing_instructions</code> \u2014 namely, after the <code>ChartGeneratorWorkflow</code> finishes executing, we told the router to terminate the Beehive if it noticed a <code>ModuleNotFoundError</code>.</p> <p>Note</p> <p>When a question is posed to a nested Beehive, the nested Beehive will use its router to determine which of its member invokables would be best suited to answer the question.</p>"},{"location":"core_concepts/beehives/#higher-order-invokables","title":"Higher-order invokables","text":"<p>So far, the Beehives we have looked at have just used <code>BeehiveAgents</code> (or <code>BeehiveLangchainAgents</code>). You can use more complex invokables inside your Beehives as well.</p> <p>For example, here is a <code>Beehive</code> that uses the <code>BeehiveDebateTeam</code> as one of its invokables:</p> <pre><code># Invokable for planning our essay -- this is a BeehiveDebateTeam, so it will spawn\n# multiple agents and that debate it out with one another. Note that these teams\n# take longer to invoke than regular agents.\nplanner = BeehiveDebateTeam(\n    num_members=4,\n    name=\"EssayPlanner\",\n    backstory=(\n        \"You are an expert essay outliner. You create thoughtful, logical, structured essay outlines.\"\n        \" You DO NOT actually write essays, you simply create an outline.\",\n    ),\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    num_rounds=2,\n    debate_length=\"short\",\n    judge_model=OpenAIModel(\n        model=\"gpt-3.5-turbo\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n)\n\n# Agent for researching topics related to the subjet matter\nresearcher = BeehiveAgent(\n    name=\"Researcher\",\n    backstory=\"You are a researcher that has access to the web via your tools.\",\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    tools=[tavily_search_tool],\n)\n\n# Invokable for writing our essay -- this is a BeehiveEnsemble, so it will spawn\n# multiple agents and summarize the various answers. Note that these, like\n# BeehiveDebateTeams take longer to invoke than regular agents.\nwriters = BeehiveEnsemble(\n    num_members=4,\n    name=\"WritingEnsemble\",\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    backstory=\"You are an expert author. You specialize are writing clear, evidence-based non-fiction writing essays.\",\n    final_answer_method=\"llm\",\n    synthesizer_model=OpenAIModel(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    )\n)\n\n# Use a FixedExecution for now.\nessay_workflow = Beehive(\n    name=\"EssayWorkflow\",\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    execution_process=FixedExecution(route=planner &gt;&gt; new_researcher &gt;&gt; writers),\n)\nessay_workflow.invoke((\n    \"Write a short, 1-paragraph essay analyzing the impact of social media on political discourse\"\n    \" in the 21st century, providing specific examples and supporting evidence from credible sources.\"\n))\n</code></pre> Beehive with Ensembles / DebaterTeams <pre><code>------------------------------------------------------------------------------------------------------------------------\nEssayWorkflow / EssayPlanner\n\nCreate a detailed essay outline analyzing the impact of social media on political discourse in the 21st century. Include\nsections for introduction, overview of social media's role in shaping political discussions, specific examples of social\nmedia platforms influencing political narratives, effects on public opinion and polarization, and a conclusion\nsummarizing key points. Each section should have bullet points outlining main ideas and supporting evidence from\ncredible sources.\n\nI. Introduction\n- Brief overview of the rise of social media in the 21st century\n- Importance of political discourse in society\n- Thesis statement: Social media has significantly impacted political discourse in the 21st century by shaping\ndiscussions, influencing narratives, affecting public opinion, and contributing to polarization.\n\nII. Overview of Social Media's Role in Shaping Political Discussions\n- Definition of political discourse in the context of social media\n- Accessibility and reach of social media platforms\n- Speed of information dissemination\n- Ability to engage with diverse audiences\n- Credibility concerns and misinformation (Sources: Pew Research Center, Harvard Kennedy School Misinformation Review)\n\nIII. Specific Examples of Social Media Platforms Influencing Political Narratives\nA. Twitter\n1. Hashtags and trending topics\n2. Direct communication from political figures\nB. Facebook\n1. Targeted advertising and political campaigns\n2. Spread of fake news and echo chambers\nC. Instagram\n1. Visual storytelling and political activism\n2. Influencer endorsements and political messaging\n\nIV. Effects on Public Opinion and Polarization\n- Echo chambers and filter bubbles\n- Amplification of extreme viewpoints\n- Disinformation campaigns and foreign interference\n- Increased engagement and political activism (Source: Journal of Communication)\n\nV. Conclusion\n- Recap of social media's impact on political discourse\n- Call for critical thinking and media literacy\n- Suggestions for regulating social media platforms to promote healthy political discussions (Sources: Brookings\nInstitution, Journal of Communication)\n\nIn the final analysis, it is evident that social media has played a pivotal role in shaping political discourse in the\n21st century. The accessibility, speed of information dissemination, and ability to engage diverse audiences have\ntransformed how political discussions unfold. However, concerns regarding credibility, misinformation, and the\namplification of extreme viewpoints highlight the need for critical thinking and media literacy among users. Regulation\nof social media platforms is essential to foster healthy political discussions and mitigate the negative impacts of\npolarization and misinformation.\n------------------------------------------------------------------------------------------------------------------------\nEssayWorkflow / Router\n\nSending conversation to Researcher next!\n------------------------------------------------------------------------------------------------------------------------\nEssayWorkflow / Researcher\n\nResearch and compile specific examples of social media platforms influencing political narratives in the 21st century.\nUtilize credible sources to gather evidence supporting the impact of social media on political discourse. Focus on the\nrole of platforms like Twitter, Facebook, and Instagram in shaping political discussions.\n\nTwitter has had a significant impact on political discourse in the 21st century, contributing to social and political\npolarization, public policies, and electoral outcomes. While it provides a vibrant online space for political\ndiscussions, it has also been associated with fostering a culture of hostility and incivility, with users engaging in\ninsulting or attacking behavior with little fear of consequences. Twitter has become a platform for the dissemination of\npolitical messages charged with polarity, influencing how politicians, governments, and individuals interact and engage\nwith one another. The impact of Facebook on political discourse in the 21st century has been significant, as evidenced\nby studies showing that exposure to political information on Facebook can increase issue salience among participants,\nparticularly those with low political interest. Social media, including Facebook, has transformed the dynamics of\npolitical activism by facilitating rapid dissemination of information and mobilization of individuals, revolutionizing\nhow movements form and influence societal change. Social media platforms have reshaped how individuals engage with\npolitical issues, voice their opinions, and mobilize for change, highlighting the profound impact of social media on\npolitical discourse in shaping public opinion and influencing elections. The impact of Instagram on political discourse\nin the 21st century has been significant, with social media platforms like Instagram playing a crucial role in shaping\nsocietal trends and political interactions. Social media has altered how politicians, governments, and individuals\nengage with one another, influencing the nature of political discourse. However, it is important to note that social\nmedia platforms can also contribute to political polarization and the spread of misinformation, which can hinder\nconstructive dialogue and debate.\n------------------------------------------------------------------------------------------------------------------------\nEssayWorkflow / Router\n\nSending conversation to WritingEnsemble next!\n------------------------------------------------------------------------------------------------------------------------\nEssayWorkflow / WritingEnsemble\n\nWrite a concise, evidence-based essay analyzing the impact of social media on political discourse in the 21st century.\nBegin by introducing the pervasive role of social media in modern society and how it has transformed communication and\ninformation dissemination. Explore both the positive effects, such as facilitating political engagement and access to\ndiverse perspectives, and negative effects, like the spread of misinformation and polarization. Provide specific\nexamples of platforms like Twitter contributing to social and political polarization, Facebook increasing issue salience\namong users, and Instagram shaping societal trends. Support your analysis with credible sources and data to illustrate\nthe profound influence of social media on political discourse.\n\n**Title: The Impact of Social Media on Political Discourse in the 21st Century**\n\n**Introduction**\nSocial media has revolutionized communication and information dissemination in modern society, profoundly influencing\npolitical discourse in the 21st century. It has redefined how individuals engage with political issues, express their\nopinions, and mobilize for change.\n\n**Positive Effects of Social Media on Political Discourse**\n- **Facilitation of Political Engagement**: Social media platforms have empowered individuals to participate in\npolitical discussions and activism, exemplified by movements like #BlackLivesMatter and #MeToo.\n- **Access to Diverse Perspectives**: Social media provides access to a wide array of political viewpoints and news\nsources, fostering critical thinking and a more informed citizenry.\n\n**Negative Effects of Social Media on Political Discourse**\n- **Spread of Misinformation**: Social media's role in disseminating fake news and misinformation has influenced public\nperception and decision-making processes.\n- **Polarization and Echo Chambers**: Algorithms on social media platforms contribute to the formation of echo chambers,\nreinforcing existing beliefs and exacerbating political polarization.\n\n**Specific Examples of Social Media Platforms Impacting Political Discourse**\n- **Twitter**: Known for fostering social and political polarization, Twitter hosts interactions that can amplify\nsocietal divisions and influence public opinion.\n- **Facebook**: Exposure to political information on Facebook can heighten issue salience among users, particularly\nthose with low political interest, transforming political activism and mobilization.\n- **Instagram**: While shaping societal trends and political interactions, Instagram's influence on political discourse\ncan also contribute to polarization and the spread of misinformation.\n\n**Conclusion**\nThe impact of social media on political discourse is multifaceted, offering opportunities for political engagement and\naccess to diverse perspectives while also posing challenges such as misinformation and polarization. Understanding these\ndynamics is crucial for navigating the evolving landscape of digital communication and ensuring responsible engagement\nwith social media in shaping public opinion and political outcomes.\n\n**References**\n- Pew Research Center. (2021). The Role of Social Media in American Politics.\n- Harvard Kennedy School. (2020). Misinformation and Polarization on Social Media.\n- Journal of Communication. (2019). The Influence of Social Media on Political Participation and Civic Engagement.\n</code></pre> <p>This wasn't perfect \u2014\u00a0in our original task, we instructed our Beehive to write a 1-paragraph essay. The final essay is definitely much longer. To correct this, we could do a number of things: - Modify our prompts - Modify our Beehive conversation to QC the essay plan and compositions (e.g., via critic invokables)</p>"},{"location":"core_concepts/chat_models/","title":"Chat Models","text":""},{"location":"core_concepts/chat_models/#overview","title":"Overview","text":"<p>Chat models are the engine that power all invokable classes. When a user invokes an <code>Invokable</code>, the task is passed through to the underlying chat model. The chat model executes the task according to the <code>Invokable</code>'s persona, history, and configuration, and then passes the result back to the Invokable.</p>"},{"location":"core_concepts/chat_models/#attributes","title":"Attributes","text":"<p><code>BHChatModels</code> are fairly simple \u2014 they are meant to be a light wrapper around the underling LLM provider's client:</p> Attribute<code>type</code> Description model<code>str</code> LLM model, e.g., <code>gpt-3.5-turbo</code>. **model_config Additional keyword arguments accepted by the LLM provider's client. <p>Chat models can be used via the <code>chat</code> or <code>stream</code> methods:</p> <pre><code>class BHChatModel(BaseModel):\n    ...\n    def chat(\n        self,\n        task_message: BHMessage | None,\n        temperature: int,\n        tools: dict[str, BHTool],\n        conversation: list[BHMessage | BHToolMessage],\n    ) -&gt; list[BHMessage | BHToolMessage]:\n        ...\n\n    def stream(\n        self,\n        task_message: BHMessage | None,\n        temperature: int,\n        tools: dict[str, BHTool],\n        conversation: list[BHMessage | BHToolMessage],\n        printer: Optional[\"Printer\"] = None,\n    ) -&gt; list[BHMessage | BHToolMessage]:\n        ...\n</code></pre> <p>These methods have nearly identical definitions. The <code>stream</code> method has one additional argument <code>printer</code>:</p> Argument Description task_message User message containing the task. If this message does not have a 'user' role, Beehive will throw an error. temperature Temperature setting for underlying LLM. tools Any tools to prove the LLM. Note that the language model must support tool calling in order to properly make use of tools. conversation List of messages that the LLM should treat as context for the current task. printer<code>stream</code> only <code>output.printer.Printer</code> instance. Used to prettify the streamed output. <p>The supported chat models are listed below.</p>"},{"location":"core_concepts/chat_models/#openai","title":"OpenAI","text":"<p>Beehive's <code>OpenAIModel</code> class provides a thin wrapper over the <code>openai.OpenAI</code> Python client:</p> <pre><code>from beehive.message import BHMessage, MessageRole\nfrom beehive.models.openai_model import OpenAIModel\n\nopenai_chat_model = OpenAIModel(\n    model=\"gpt-3.5-turbo\",\n    api_key=\"&lt;your_api_key\"&gt;,  # keyword argument accepted by openai.OpenAI client\n    max_retries=10,  # keyword argument accepted by openai.OpenAI client\n)\njoke = openai_chat_model.chat(\n    input_messages=[\n        BHMessage(\n            role=MessageRole.USER,\n            content=\"Tell me a joke!\"\n        )\n    ],\n    temperature=0,\n    tools={},\n    conversation=[],\n)\nprint(joke)\n</code></pre> <p>This will print the following: <pre><code>[BHMessage(role=&lt;MessageRole.ASSISTANT: 'assistant'&gt;, content=\"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two tired!\", tool_calls=[])]\n</code></pre></p> <p>Warning</p> <p>You can certainly chat with <code>BHChatModels</code> directly. However, doing so require that you use Beehive-native types. We highly recommend using Invokables instead \u2014 these are simpler to work with \"out-of-the-box\".</p>"},{"location":"core_concepts/feedback/","title":"Feedback","text":"<p>Feedback enables Beehive <code>Invokables</code> to preserve valuable insights and learnings from past executions, allowing agents to build and refine their knowledge over time.</p> <p>Feedback can be configured with the following invokable attributes:</p> Attribute<code>type</code> Description feedback<code>bool</code> Whether to use feedback from the invokable's previous interactions. Feedback enables the LLM to improve their responses over time. Note that only feedback from tasks with a similar embedding are used. feedback_embedder<code>BHEmbeddingModel | None</code> Embedding model used to calculate embeddings of tasks. These embeddings are stored in a vector database. When a user prompts the Invokable, the Invokable searches against this vector database using the task embedding. It then takes the suggestions generated for similar, previous tasks and concatenates them to the task prompt. Default is <code>None</code>. feedback_prompt_template<code>str | None</code> Prompt for evaluating the output of an LLM. Used to generate suggestions that can be used to improve the model's output in the future. If <code>None</code>, then Beehive will use the default prompt template. Default is <code>None</code>. feedback_model<code>BHChatModel | BaseChatModel</code> Language model used to generate feedback for the invokable. If <code>None</code>, then default to the Invokable's <code>model</code> attribute. feedback_embedding_distance<code>EmbeddingDistance</code> Distance method of the embedding space. See the ChromaDB documentation for more information: https://docs.trychroma.com/guides#changing-the-distance-function. n_feedback_results<code>int</code> Amount of feedback to incorporate into answering the current task. This takes <code>n</code> tasks with the most similar embedding to the current one and incorporates their feedback into the Invokable's model. Default is <code>1</code>. <p>When <code>feedback=True</code>, here's what happens under the hood:</p> <ul> <li>When an <code>Invokable</code> is invoked with a task, Beehive embeds the task using the embedding model provided in <code>feedback_embedder</code></li> <li>Beehive then searches in a vector database (ChromaDB) for similar tasks. The number of similar tasks is determined by <code>n_feedback_results.</code></li> <li>Beehive grabs the feedback for those like tasks, concatenates it into a single message, and augments the <code>Invokable</code> state with that feedback.</li> <li>Beehive then invokes the <code>Invokable</code>.</li> </ul> <p>Note</p> <p>Like the SQLite database powering <code>history</code>, the Chroma database containing task feedback lives in <code>~/.beehive</code>.</p>"},{"location":"core_concepts/feedback/#bhembeddingmodel","title":"BHEmbeddingModel","text":"<p>The <code>feedback_embedder</code> is a core part of Beehive's feedback process. This model is an instance of the <code>BHEmbeddingModel</code> base class.</p> <p>Like <code>BHChatModels</code> <code>BHEmbeddingModels</code> are fairly simple. In fact, the attributes are the exact same:</p> Attribute<code>type</code> Description model<code>str</code> Open AI embedding model. Default is <code>text-embedding-3-small</code>. **model_config Additional keyword arguments accepted by the LLM provider's client. <p>Embedding models can be used via the <code>get_embeddings</code>:</p> <pre><code>class BHEmbeddingModel:\n    def get_embeddings(self, text: str) -&gt; Sequence[float]:\n        ...\n</code></pre> <p>This method takes as input some text and returns a sequence of floats.</p>"},{"location":"core_concepts/feedback/#openaiembedder","title":"OpenAIEmbedder","text":"<p>Beehive's <code>OpenAIEmbedder</code> class provides a thin wrapper over the <code>openai.OpenAI</code> Python client. The available embedding models can be found here.</p> <pre><code>from beehive.message import BHMessage, MessageRole\nfrom beehive.models.openai_embedder import OpenAIEmbedder\n\nembedding_model = OpenAIEmbedder(\n    model=\"text-embedding-3-small\",\n    api_key=\"&lt;your_api_key\"&gt;,  # keyword argument accepted by openai.OpenAI client\n    max_retries=10,  # keyword argument accepted by openai.OpenAI client\n)\nembeddings = embedding_model.get_embeddings(text=\"Embed this, please!\")\nprint(embeddings)\n# [\n#   -0.015081570483744144,\n#   -0.005574650131165981,\n#   -0.023674558848142624,\n#   0.0027788940351456404,\n#   -0.013219981454312801,\n#   -0.029758447781205177,\n#   -0.01786046475172043,\n#   -0.010474811308085918,\n#   -0.04527169093489647,\n#   -0.01892615668475628,\n#   0.020275134593248367,\n# ...\n# ]\n</code></pre>"},{"location":"core_concepts/invokables/","title":"Invokables","text":"<p>Note</p> <p>You will never need to instantiate this class directly. You should always use one of the child classes.</p> <p><code>Invokables</code> are a core construct in Beehive. An <code>Invokable</code> is anything that uses an LLM in its internal architecture to reason through and execute a user's task.</p>"},{"location":"core_concepts/invokables/#base-attributes","title":"Base Attributes","text":"<p>Info</p> <p>Note that the <code>Invokable</code> class is a Pydantic <code>BaseModel</code>.</p> Attribute<code>type</code> Description name<code>str</code> The invokable name. backstory<code>str</code> Backstory for the AI actor. This is used to prompt the AI actor and direct tasks towards it. Default is: 'You are a helpful AI assistant.' model<code>BHChatModel | BaseChatModel</code> Chat model used by the invokable to execute its function. This can be a <code>BHChatModel</code> or a Langchain <code>ChatModel</code>. chat_loop<code>int</code> Number of times the model should loop when responding to a task. Usually, this will be 1, but certain prompting patterns may require more loops (e.g., chain-of-thought prompting). state<code>list[BHMessage | BHToolMessage] | list[BaseMessage]</code> List of messages that this actor has seen. This enables the actor to build off of previous conversations / outputs. history<code>bool</code> Whether to use previous interactions / messages when responding to the current task. Default is <code>False</code>. history_lookback<code>int</code> Number of days worth of previous messages to use for answering the current task. feedback<code>bool</code> Whether to use feedback from the invokable's previous interactions. Feedback enables the LLM to improve their responses over time. Note that only feedback from tasks with a similar embedding are used. feedback_embedder<code>BHEmbeddingModel | None</code> Embedding model used to calculate embeddings of tasks. These embeddings are stored in a vector database. When a user prompts the Invokable, the Invokable searches against this vector database using the task embedding. It then takes the suggestions generated for similar, previous tasks and concatenates them to the task prompt. Default is <code>None</code>. feedback_model<code>BHChatModel | BaseChatModel</code> Language model used to generate feedback for the invokable. If <code>None</code>, then default to the <code>model</code> attribute. feedback_embedding_distance<code>EmbeddingDistance</code> Distance method of the embedding space. See the ChromaDB documentation for more information: https://docs.trychroma.com/guides#changing-the-distance-function. n_feedback_results<code>int</code> Amount of feedback to incorporate into answering the current task. This takes <code>n</code> tasks with the most similar embedding to the current one and incorporates their feedback into the Invokable's model. Default is <code>1</code>. color<code>str</code> Color used to represent the invokable in verbose printing. This can be a HEX code, an RGB code, or a standard color supported by the Rich API. See https://rich.readthedocs.io/en/stable/appendix/colors.html for more details. Default is <code>chartreuse2</code>."},{"location":"core_concepts/invokables/#invoke-method","title":"\"invoke\"  method","text":"<p>In order to have your invokable execute a task, you can use the <code>invoke</code> method. You'll see several examples of this throughout the documentation.</p> Argument<code>type</code> Description task<code>str</code> Task to execute. retry_limit<code>str</code> Maximum number of retries before the Invokable returns an error. Default is <code>100</code>. pass_back_model_errors<code>bool</code> Boolean controlling whether to pass the contents of an error back to the LLM via a prompt.  Default is <code>False</code>. verbose<code>bool</code> Beautify stdout logs with the <code>rich</code> package. Default is <code>True</code>. context<code>list[Invokable] | None</code> List of Invokables whose state should be treated as context for this invokation. stream<code>bool</code> Stream the output of the agent character-by-character. Default is <code>False</code>. stdout_printer<code>output.printer.Printer | None</code> Printer object to handle stdout messages. Default is <code>None</code>. <p>Beehive offers several invokables out-of-the-box:</p> <ul> <li><code>BeehiveAgent</code></li> <li><code>BeehiveLangchainAgent</code></li> <li><code>BeehiveEnsemble</code></li> <li><code>BeehiveDebate</code></li> </ul> <p>We'll cover these in detail next.</p>"},{"location":"core_concepts/invokables/#beehiveagent","title":"<code>BeehiveAgent</code>","text":"<p><code>BeehiveAgent</code>s are the most basic type of <code>Invokable</code>. They are autonomous units programmed to execute complex tasks by combining tool usage and memory.</p> <p>Here are the additional fields supported by the <code>BeehiveAgent</code> class.</p> Argument<code>type</code> Description temperature<code>int</code> Temperature setting for the agent's chat model tools<code>list[Callable[..., Any]]</code> Functions that this agent can use to answer questions. These functions are converted to tools that can be intepreted and executed by LLMs. Note that the language model must support tool calling for these tools to be properly invoked. docstring_format<code>DocstringFormat | None</code> Docstring format in functions. Beehive uses these docstrings to convert functions into LLM-compatible tools. If <code>None</code>, then Beehive will autodetect the docstring format and parse the arg descriptions. Default is <code>None</code>. <p>Warning</p> <p>Note that <code>tools</code> is simply a list of functions. These functions should have docstrings and type-hints. Beehive will throw an error if either of these are missing.</p> <pre><code>from beehive.invokable.agent import BeehiveAgent\nfrom beehive.models.openai_model import OpenAIModel\n\nmath_agent = BeehiveAgent(\n    name=\"MathAgent\",\n    backstory=\"You are a helpful AI assistant. You specialize in performing complex calculations.\",\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    tools=[],\n    history=True,\n    feedback=True,\n)\nmath_agent.invoke(\"What's 2+2?\")\n</code></pre>"},{"location":"core_concepts/invokables/#beehivelangchainagent","title":"<code>BeehiveLangchainAgent</code>","text":"<p><code>BeehiveLangchainAgents</code> are similar to <code>BeehiveAgents</code>, except they use Langchain-native types internally.</p> <p>Here are the additional fields supported by the <code>BeehiveLangchainAgent</code> class.</p> Argument<code>type</code> Description temperature<code>int</code> Temperature setting for the agent's chat model tools<code>list[Callable[..., Any]]</code> Functions that this agent can use to answer questions. These functions are converted to tools that can be intepreted and executed by LLMs. Note that the language model must support tool calling for these tools to be properly invoked. docstring_format<code>DocstringFormat | None</code> Docstring format in functions. Beehive uses these docstrings to convert functions into LLM-compatible tools. If <code>None</code>, then Beehive will autodetect the docstring format and parse the arg descriptions. Default is <code>None</code>. config<code>RunnableConfig | None</code> Langchain Runnable configuration. This is used inside the ChatModel's <code>invoke</code> method. Default is <code>None</code>. stop<code>list[str]</code> List of strings on which the model should stop generating. **model_kwargs Extra keyword arguments for invoking the Langchain chat model. <pre><code>from beehive.invokable.langchain_agent import BeehiveLangchainAgent\nfrom langchain_openai.chat_models import ChatOpenAI\n\nmath_agent = BeehiveLangchainAgent(\n    name=\"MathAgent\",\n    backstory=\"You are a helpful AI assistant. You specialize in performing complex calculations.\",\n    model=ChatOpenAI(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    tools=[],\n    history=True,\n    feedback=True,\n)\nmath_agent.invoke(\"What's 2+2?\")\n</code></pre>"},{"location":"core_concepts/invokables/#beehiveensemble","title":"<code>BeehiveEnsemble</code>","text":"<p>In a <code>BeehiveEnsemble</code>, <code>n</code> agents are given the same task and produce <code>n</code> different responses. These responses are then synthesized together to produce a final answer.</p> <p>Beehive currently supports two different synthesis methods: an LLM agent or a similarity function. In the former, Beehive creates a new LLM agent whose task is to combine all <code>n</code> responses into a better, final response. In the latter, Beehive computes the similarity between all pairs of responses and returns the answer that had the highest cumulative similarity.</p> <p>Here are the additional fields supported by the <code>BeehiveEnsemble</code> class.</p> Argument<code>type</code> Description temperature<code>int</code> Temperature setting for the agent's chat model tools<code>list[Callable[..., Any]]</code> Functions that this agent can use to answer questions. These functions are converted to tools that can be intepreted and executed by LLMs. Note that the language model must support tool calling for these tools to be properly invoked. docstring_format<code>DocstringFormat | None</code> Docstring format in functions. Beehive uses these docstrings to convert functions into LLM-compatible tools. If <code>None</code>, then Beehive will autodetect the docstring format and parse the arg descriptions. Default is <code>None</code>. num_members<code>int</code> Number of members on the team. final_answer_method<code>Literal['llm', 'similarity']</code> Method used to obtain the final answer from the agents. Either <code>llm</code> or <code>similarity</code>. If <code>llm</code>, then Beehive will create an agent with the inputted <code>synthesizer_model</code> and use that to synthesize the responses from the agents and generate a single, final response. If <code>similarity</code>, then Beehive will choose the answer that has the highest cumulative similarity to the other agents. synthesizer_model<code>BHChatModel | BaseChatModel | None</code> Model used to synthesize responses from agents and generate a final response. Only necessary if <code>final_answer_method</code>='llm'. This class must match the <code>model</code> class. similarity_score_func<code>Callable[[str, str], float]</code> Function used to compute the similarity score. Only necessary if <code>final_answer_method</code>='similarity'. The function must take two string arguments and return a float. If the callable is not specified, then Beehive defaults to the BLEU score from Papineni et al., 2002. Default is <code>None</code>. **agent_kwargs Extra keyword arguments for agent instantiation. This is ONLY used for Langchain agents, and this is used for both the member agent and synthesizer agent instantiation. <p>This was inspired by the work of Li et. al.</p> <pre><code>from beehive.invokable.ensemble import BeehiveEnsemble\nfrom beehive.models.openai_model import OpenAIModel\n\n# Using similarity scores\nensemble_similarity = BeehiveEnsemble(\n    name=\"TestEnsembleSimilarity\",\n    backstory=\"You are an expert software engineer.\",\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    num_members=4,\n    history=True,\n    final_answer_method=\"similarity\",\n)\nensemble_similarity.invoke(\"Write a script that downloads data from S3.\")\n\n# Using synthesizer model\nensemble_synthesizer = BeehiveEnsemble(\n    name=\"TestEnsembleSimilarity\",\n    backstory=\"You are an expert software engineer.\",\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    num_members=4,\n    history=True,\n    final_answer_method=\"llm\",\n    synthesizer_model=OpenAIModel(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    )\n)\nensemble_similarity.invoke(\"Write a script that uploads data from S3.\")\n</code></pre>"},{"location":"core_concepts/invokables/#beehivedebateteam","title":"<code>BeehiveDebateTeam</code>","text":"<p>In an <code>BeehiveDebateTeam</code>, <code>n</code> agents are initially given the same task and produce <code>n</code> different responses. The agents then \"debate\" with one another, i.e., they look at the output of the other <code>n-1</code> agents and update their own response. This happens over several rounds. Finally, a \"judge\" (another LLM agent) evaluates all of the responses and chooses the one answer the initial query best.</p> <p>Here are the additional fields supported by the <code>BeehiveDebateTeam</code> class.</p> Argument<code>type</code> Description temperature<code>int</code> Temperature setting for the agent's chat model tools<code>list[Callable[..., Any]]</code> Functions that this agent can use to answer questions. These functions are converted to tools that can be intepreted and executed by LLMs. Note that the language model must support tool calling for these tools to be properly invoked. docstring_format<code>DocstringFormat | None</code> Docstring format in functions. Beehive uses these docstrings to convert functions into LLM-compatible tools. If <code>None</code>, then Beehive will autodetect the docstring format and parse the arg descriptions. Default is <code>None</code>. num_members<code>int</code> Number of members on the team. num_rounds<code>int</code> Number of debate rounds. judge_model<code>BHChatModel | BaseChatModel</code> Model used to power the judge agent. **agent_kwargs Extra keyword arguments for agent instantiation. This is ONLY used for Langchain agents, and this is used for both the member agent and synthesizer agent instantiation. <p>This was inspired by the work of Du et. al.</p> <pre><code>from beehive.invokable.debate import BeehiveDebateTeam\nfrom beehive.models.openai_model import OpenAIModel\n\ndebaters = BeehiveDebateTeam(\n    name=\"TestDebateTeam\",\n    backstory=\"You are a helpful AI assistant.\",\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    num_members=2,\n    num_rounds=2,\n    judge_model=OpenAIModel(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    history=True,\n    feedback=True,\n)\ndebaters.invoke(\" A treasure hunter found a buried treasure chest filled with gems. There were 175 diamonds, 35 fewer rubies than diamonds, and twice the number of emeralds than the rubies. How many of the gems were there in the chest?\")\n</code></pre>"},{"location":"core_concepts/invokables/#creating-your-own","title":"Creating your own","text":"<p>Earlier, we talked about how one can use the <code>invoke</code> method to actually execute the invokable. The internal logic for this is determined by the <code>_invoke</code> method. In order to create your own invokable, one simply needs to create a class that inherits the <code>Invokable</code> base class and implement this <code>_invoke</code> method.</p> <p>Here's what this method looks like. Note the differences between this and <code>invoke</code>:</p> <pre><code>class Invokable(BaseModel):\n    ...\n\n    def _invoke(\n        self,\n        task: str,\n        retry_limit: int = 100,\n        pass_back_model_errors: bool = False,\n        verbose: bool = True,\n        stream: bool = False,\n        stdout_printer: Printer | None = None,\n    ) -&gt; list[Any]:\n        \"\"\"Invoke the Invokable to execute a task.\n\n        args:\n        - `task` (str): task to execute.\n        - `retry_limit` (int): maximum number of retries before the Invokable returns an error. Default is `100`.\n        - `pass_back_model_errors` (bool): boolean controlling whether to pass the contents of an error back to the LLM via a prompt.  Default is `False`.\n        - `verbose` (bool): beautify stdout logs with the `rich` package. Default is `True`.\n        - `stream` (bool): stream the output of the agent character-by-character. Default is `False`.\n        - `stdout_printer` (`output.printer.Printer` | None): Printer object to handle stdout messages. Default is `None`.\n\n        returns:\n        - list[BHMessage | BHToolMessage] | list[BaseMessage] | list[BHStateElt]\n        \"\"\"\n        raise NotImplementedError()\n</code></pre> <p>Here's an example of a custom RAG invokable:</p> Custom RAG Invokable <p>In this example, we show the implementation of a custom RAG invokable. Note that this invokable assumes that you've already set up a vector store with your chunked document embeddings.</p> <p>Warning</p> <p>This is just for illustrative purposes. This has not been tested!</p> <pre><code>from beehive.invokable.base import Invokable\nfrom beehive.message import BHMessage, BHToolMessage, MessageRole\nfrom chromadb import PersistentClient, Collection\nfrom openai import OpenAI\nfrom pydantic import Field, PrivateAttr, model_validator\n\n\nclass CustomRAGInvokable(Invokable):\n    model_config = ConfigDict(extra=\"allow\")\n\n    client: Any = Field(\n        description=\"ChromaDB `PersistentClient` object\"\n    )\n    embedding_model: str = Field(\n        description=\"Embedding model used to create document embeddings.\",\n        default=\"text-embedding-3-small\",\n    )\n    n_results: int = Field(\n        description=\"Number of documents to retrieve.\",\n        default=10,\n    )\n\n    # Embedding client and collection containing documents\n    _embedding_client: OpenAI = PrivateAttr()\n    _document_collection: Collection = PrivateAttr()\n\n    @model_validator(mode=\"after\")\n    def define_document_collection(self) -&gt; \"CustomRAGInvokable\":\n        self._embedding_client = OpenAI(**self.model_extra if self.model_extra else {})\n\n        # Type-check the client keyword-argument\n        if not isinstance(self.client, PersistentClient):\n            raise ValueError(\"`client` must be a `PersistentClient`!\")\n        self._document_collection = self.client.get_or_create_collection(\n            name=\"documents\", metadata={\"hnsw:space\": \"l2\"}\n        )\n        return self\n\n    def _invoke(\n        self,\n        task: str,\n        context: list[\"Invokable\"] | None = None,\n        feedback: BHMessage | None = None,\n        retry_limit: int = 100,\n        pass_back_model_errors: bool = False,\n        verbose: bool = True,\n        stream: bool = False,\n        stdout_printer: Printer | None = None,\n    ) -&gt; list[BHMessage | BHToolMessage]:\n        \"\"\"For this simple RAG invokable, we will:\n        - Embed the task\n        - Grab the documents most similar to this task\n        - Create a prompt for the model\n        - Return the output\n        \"\"\"\n        task_embeddings = (\n            self._embedding_client.embeddings.create(input=[task], model=self.embedding_model)\n            .data[0]\n            .embedding\n        )\n        documents = self._document_collection.query(\n            query_embeddings=[embeddings],\n            n_results=self.n_results,\n        )\n\n        # Handle our context and feedback\n\n        # Construct the prompt\n        document_context = \"\\n\\n\".join(documents[\"documents\"])\n        prompt = f\"{task}\\nHere is some context to help you answer this question:\\n&lt;context&gt;{document_context}&lt;/context&gt;\"\n        task_message = BHMessage(\n            role=MessageRole.USER,\n            content=prompt\n        )\n\n        output = self.model.chat(\n            input_messages=[task_message],\n            temperature=self.temperature,\n            tools={},\n            conversation=[]\n        )\n\n        # Add the model's response to the agent's tate\n        self.state.extend(output)\n\n        # Return\n        return output\n</code></pre> <p>Here's exactly what we did:</p> <ul> <li>Created a class <code>CustomRAGInvokable</code> that inherits from the <code>Invokable</code> class</li> <li>Endowed this class with custom fields that are specific to our RAG implementation. The <code>Invokable</code> class is a Pydantic <code>BaseModel</code>, so runtime type-checking is automatically enforced.</li> <li>Added some light model validation and field instantiation.</li> <li>Implement the <code>_invoke</code> method.</li> </ul>"},{"location":"core_concepts/invokables/#coming-soon","title":"Coming soon!","text":"<p>Here are some additional invokables we're actively developing:</p> <ul> <li><code>BeehiveCOT</code></li> <li><code>BeehiveReflexion</code></li> </ul>"},{"location":"core_concepts/memory/","title":"Memory","text":"<p>Beehive supports both short-term memory and long-term memory.</p>"},{"location":"core_concepts/memory/#short-term-memory","title":"Short-term memory","text":"<p>Short-term memory allows <code>Invokables</code> to temporarily store recent interactions and outcomes using RAG, enabling agents to recall and utilize information relevant to their current context during the current executions.</p> <p>This memory is handled by the <code>state</code> attribute in Invokables. <code>state</code> is a list of messages (e.g., <code>BHMessage | BHToolMessage</code> or <code>BaseMessage</code> depending on the type of <code>Invokable</code>).</p> <p>Wnen an Invokable is instantiated, <code>state</code> is instantiated as an empty list. Whenever a user invokes the invokable, Beehive automatically stores the messages in the <code>state</code>. Then, when the user invoke the invokable again, this state is used as context.</p> <p>For example, for <code>Invokables</code> that use the <code>OpenAIModel</code>, the state is passed as the <code>messages</code> argument in the chat completions API.</p> <p>Here's an example: <pre><code>joke_agent = BeehiveAgent(\n    name=\"JokeAgent\",\n    backstory=\"You are an expert comedian.\",\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo-0125\",\n        api_key=\"&lt;your_api_key&gt;\",\n    )\n)\njoke_agent.invoke(\"Tell me a joke!\")\n# Why did the scarecrow win an award? Because he was outstanding in his field!\n\nprint(joke_agent.state)\n# [\n#   BHMessage(role=&lt;MessageRole.SYSTEM: 'system'&gt;, content='You are an expert comedian.', tool_calls=[]),\n#   BHMessage(role=&lt;MessageRole.USER: 'user'&gt;, content='Tell me a joke!', tool_calls=[]),\n#   BHMessage(role=&lt;MessageRole.ASSISTANT: 'assistant'&gt;, content='Why did the scarecrow win an award?\\n\\nBecause he was outstanding in his field!', tool_calls=[])\n# ]\n\njoke_agent.invoke(\"Tell me a different joke, but use the same subject!\")\n# Why did the scarecrow become a successful stand-up comedian? Because he was a master at delivering corny jokes!\n\nprint(joke_agents.state)\n# [\n#   BHMessage(role=&lt;MessageRole.SYSTEM: 'system'&gt;, content='You are an expert comedian.', tool_calls=[]),\n#   BHMessage(role=&lt;MessageRole.USER: 'user'&gt;, content='Tell me a joke!', tool_calls=[]),\n#   BHMessage(role=&lt;MessageRole.ASSISTANT: 'assistant'&gt;, content='Why did the scarecrow win an award?\\n\\nBecause he was outstanding in his field!', tool_calls=[])\n#   BHMessage(role=&lt;MessageRole.USER: 'user'&gt;, content='Tell me a different joke, but use the same subject!', tool_calls=[]),\n#   BHMessage(role=&lt;MessageRole.ASSISTANT: 'assistant'&gt;, content='Why did the scarecrow become a successful stand-up comedian?\\n\\nBecause he was a master at corny jokes!', tool_calls=[])\n# ]\n</code></pre></p> <p>As you can see, the <code>state</code> agent is preserved for the life of the <code>joke_agent</code>. Moreover, the output of the first invokation is used as context for the second invokation.</p>"},{"location":"core_concepts/memory/#long-term-memory","title":"Long-term memory","text":"<p>Whereas short-term memory only exists for the life of the <code>Invokable</code>, Long-term memory enables access invokations from previous days, weeks, and months.</p> <p>When you create and invoke your first <code>Invokable</code>, Beehive creates a memory store in your local filesystem at <code>~/.beehive</code>. This memory store contains two files: <code>beehive.db</code> and <code>feedback/</code>. The latter is used for feedback, which we'll cover in a bit.</p> <p>Long-term memory is controlled by two attributes: <code>history</code>, and <code>history_lookback</code>.</p> <p>When <code>history=True</code>, Beehive augments the <code>Invokable</code> state with messages from <code>history_lookback</code> days before.</p> <p>Suppose you are invoking an agent daily via some sort of CRON job. Each day, the agent performs a specific task (e.g., summarize today's news). In this case, you may want to set <code>history=True</code> and <code>num_lookback_days=7</code>, since specific news stories may evolve over time:</p> <pre><code># Day 1\nnews_agent = BeehiveAgent(\n    name=\"NewsAgent\",\n    backstory=\"You are an expert reporter that specializes in summarizing major news stories\",\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    tools=[\n        parse_nyt_news_stories,\n        parse_wsj_news_stories,\n        parse_ft_news_stories,\n    ],  # functions to grab news stories from various sources\n    history=True,\n    num_lookback_days=7,\n)\nprint(news_agent.state)\n# [\n#   BHMessage(role=&lt;MessageRole.SYSTEM: 'system'&gt;, content='You are an expert comedian.', tool_calls=[])\n# ]\n\nnews_agent.invoke(\n    task=\"Summarize the major recent U.S. news stories. Include relevant context for each news story summary (e.g., developments from previous days).\",\n)\n</code></pre> <pre><code># Day 2\nnews_agent = BeehiveAgent(\n    name=\"NewsAgent\",\n    backstory=\"You are an expert reporter that specializes in summarizing major news stories\",\n    model=OpenAIModel(\n        model=\"gpt-3.5-turbo\",\n        api_key=\"&lt;your_api_key&gt;\",\n    ),\n    tools=[\n        parse_nyt_news_stories,\n        parse_wsj_news_stories,\n        parse_ft_news_stories,\n    ],  # functions to grab news stories from various sources\n    history=True,\n    num_lookback_days=7,\n)\nprint(news_agent.state)\n# [\n#   BHMessage(role=&lt;MessageRole.SYSTEM: 'system'&gt;, content=\"You are an expert reporter that specializes in summarizing major news stories\", tool_calls=[])\n#   BHMessage(role=&lt;MessageRole.SYSTEM: 'user'&gt;, content=\"Summarize the major recent U.S. news stories. Include relevant context for each news story summary (e.g., developments from previous days).\", tool_calls=[])\n#   BHMessage(role=&lt;MessageRole.ASSISTANT: 'assistant'&gt;, content=\"Here's what you need to know about today:\", tool_calls=[\"tool_call_id1\", \"toolcall_id2\", ...])\n#   BHToolMessage(role=\"tool\", tool_call_id=\"tool_call_id1\", content=\"In the New York Times, here are the major stories:\")\n#   BHToolMessage(role=\"tool\", tool_call_id=\"tool_call_id2\", content=\"In the Wall Street Journal, here are the major stories:\")\n#   ...\n# ]\n\nnews_agent.invoke(\n    task=\"Summarize the major recent U.S. news stories. Include relevant context for each news story summary (e.g., developments from previous days).\",\n)\n</code></pre> <p>Note</p> <p>The above is example uses dummy messages / tool calls to convey how long-term memory works. It is for illustrative purposes only.</p> <p>As you can see, on day 2, the invokables state includes the messages and tool calls from the previous day.</p>"},{"location":"core_concepts/prompts/","title":"Prompts","text":"<p>Beehive's prompts control how Beehive invokes chat models, interprets feedback, and routes conversations. These templates are rendered and stored in <code>~/.beehive/prompts/</code> when you create your first invokable. Prompt templates are Jinja2 templates. Variables are represented via the <code>{{ ... }}</code> expression. These variables get populated dynamically at runtime.</p> <p>You can edit the wording of these prompts by directly modifying the text files this folder! Note that Beehive uses XML tags to structure prompts \u2014 we encourage you to maintain this style in your modifications.</p> <p>Warning</p> <p>If you modify the wording, do NOT add variables to the template. Additional variables will not be rendered properly, which may affect your invokable's behavior.</p> <p>Here is a list of Beehive's internal prompts, separated</p>"},{"location":"core_concepts/prompts/#general","title":"General","text":"<p>The following prompts are used across different kinds of invokables.</p> prompt description <code>ask_question_prompt</code> used to explicitly allow invokables to ask questions to other invokables. <code>context_prompt_concise</code> used to represent context, i.e., messages from previous invokables. This prompt is \"concise\" \u2014 i.e., it excludes information about the format for the context. <code>context_prompt_full</code> used to represent context, i.e., messages from previous invokables. This prompt icludes information about the format for the context. <code>evaluation_prompt</code> used evaluate the quality of an invokable's output. <code>feedback_prompt</code> used to incorporate feedback to an invokable's task. <code>model_error_prompt</code> used when there is an error in the agent's output. For example, if the router's output does not meet a specific format, this prompt is used to inform the router of its mistake and to try again."},{"location":"core_concepts/prompts/#beehive","title":"<code>Beehive</code>","text":"<p>The following prompts are used within the <code>Beehive</code> class.</p> prompt description <code>router_prompting_prompt</code> used by a Beehive's router agent to create prompts for invokables in a Beehive. <code>router_question_prompt</code> used by a Beehive's router agent to direct a clarification question to invokable best posed to answer the question. This is used if a <code>router_routing_prompt</code> used by a Beehive's router agent to direct the conversation to the next appropriate invokable."},{"location":"core_concepts/prompts/#beehivedebateteam","title":"<code>BeehiveDebateTeam</code>","text":"<p>The following prompts are used within the <code>BeehiveDebateTeam</code> class.</p> prompt description <code>debate_judge_prompt</code> used by the <code>BeehiveDebateTeam</code>'s <code>_judge_agent</code> when judging the debater responses. <code>debate_judge_summary_prompt</code> used to represent the final answer from <code>BeehiveDebateTeam</code>. After the <code>_judge_agent</code> judges the debate rounds, Beehive uses this template to append the final answer to each of the member agents' states. This enables the debaters to build off of previous rounds. <code>long_form_debate_prompt</code> used to prompt members of a <code>BeehiveDebateTeam</code> to another round of debate. <code>short_form_debate_prompt</code> used to prompt members of a <code>BeehiveDebateTeam</code> to another round of debate."},{"location":"core_concepts/prompts/#beehiveensemble","title":"<code>BeehiveEnsemble</code>","text":"<p>The following prompts are used within the <code>BeehiveEnsemble</code> class.</p> prompt description <code>ensemble_func_summary_prompt</code> used to represent the final answer from <code>BeehiveEnsemble</code> when the ensemble uses a <code>final_answer_method='similarity'</code> to compute the final answer. Beehive uses this template to append the final answer to each of the member agents' states. This enables the debaters to build off of previous rounds. <code>ensemble_llm_summary_prompt</code> used to represent the final answer from <code>BeehiveEnsemble</code> when the ensemble uses a <code>final_answer_method='llm'</code> to create the final answer. Beehive uses this template to append the final answer to each of the member agents' states. This enables the debaters to build off of previous rounds. <code>synthesizer_prompt</code> used to synthesize the responses of ensemble members into a single, final answer."},{"location":"core_concepts/tools/","title":"Tools","text":"<p>Tools give language models the ability to interact with the outside world. This enables agents browse the web, make requests against an API, or execute any arbitrary function.</p> <p>In Beehive, tools are simply functions with a docstring and type-hints.</p> <p>Warning</p> <p>If a function doesn't have both a docstring and type-hints, Beehive will throw an error!</p> <p>Docstrings are used to grab the tool description as well as the description for each of arguments.</p> <p>Beehive currently requires that docstring follow the sphinx or google standards. You can find the specifications here:</p> <ul> <li>Sphinx: https://sphinx-rtd-tutorial.readthedocs.io/en/latest/docstrings.html</li> <li>Google: https://google.github.io/styleguide/pyguide.html#383-functions-and-methods</li> </ul> <p>Under the hood, Beehive parses the docstring and type hint and constructs a Pydantic <code>BaseModel</code> representing the function. The serialized <code>BaseModel</code> is then passed to the <code>BHChatModel</code> powering the <code>Invokable</code>, which then expresses the intent to call a specific tool in their response. Beehive handles interpreting this intent and actually calling the function.</p> <p>Here are some examples of tools with different docstrings.</p> Sphinx docstring <pre><code>class SearchDepth(str, Enum):\n    BASIC = \"basic\"\n    ADVANCED = \"advanced\"\n\n\ndef tavily_search_tool(\n    query: str,\n    search_depth: SearchDepth = SearchDepth.BASIC,\n    include_images: bool = False,\n    include_answer: bool = True,\n    include_raw_content: bool = False,\n    max_results: int = 5,\n    include_domains: list[str] | None = None,\n    exclude_domains: list[str] | None = None,\n):\n    \"\"\"\n    Use this as a search engine optimized for comprehensive, accurate, and trusted\n    results. Very useful for when you need to answer questions about current events, or\n    if you need search the web for information.\n\n    :param query: search query\n    :type query: str\n    :param search_depth: depth of the search; basic should be used for quick results,\n        and advanced for indepth high quality results but longer response time, defaults\n        to basic\n    :type search_depth: class:`test.SearchDepth`\n    :param include_images: include a list of query related images in the response,\n        defaults to False\n    :type include_images: bool\n    :param include_answer: include answers in the search results, defaults to True\n    :type include_answer: bool\n    :param include_raw_content: include raw content in the search results, defaults to\n        False\n    :type include_raw_content: bool\n    :param max_results: number of maximum search results to return, defaults to 5.\n    :type max_results: int\n    :param include_domains: list of domains to specifically include in the search\n        results, defaults to None\n    :type include_domains: list[str], optional\n    :param exclude_domains: list of domains to specifically exclude from the search\n        results, defaults to None\n    :type exclude_domains: list[str], optional\n    \"\"\"\n    base_url = \"https://api.tavily.com/\"\n    endpoint = \"search\"\n    resp = requests.post(\n        f\"{base_url}{endpoint}\",\n        json={\n            \"api_key\": \"&lt;tavily_api_key&gt;\",\n            \"query\": query,\n            \"search_depth\": search_depth,\n            \"include_images\": include_images,\n            \"include_answer\": include_answer,\n            \"include_raw_content\": include_raw_content,\n            \"max_results\": max_results,\n            \"include_domains\": include_domains,\n            \"exclude_domains\": exclude_domains,\n        },\n    )\n    try:\n        return resp.json()[\"answer\"]\n    except json.JSONDecodeError as e:\n        logger.error(e)\n        return \"Could not execute the Tavily search...Try again!\"\n</code></pre> Google docstring <pre><code>class Gender(str, Enum):\n    male = \"male\"\n    female = \"female\"\n    other = \"other\"\n    not_given = \"not_given\"\n\n\nclass TestModel(BaseModel):\n    name: Gender = Field(default=Gender.male, description=\"test\")\n    test_object: dict[str, Any]\n\n\ndef google_fetch_smalltable_rows(\n    table_handle: TestModel,\n    keys: list[TestModel],\n    require_all_keys: bool = False,\n) -&gt; Mapping[bytes, tuple[str, ...]]:\n    \"\"\"Fetches rows from a Smalltable.\n\n    Retrieves rows pertaining to the given keys from the Table instance\n    represented by table_handle. String keys will be UTF-8 encoded.\n\n    Args:\n    table_handle:\n        An TestModel instance.\n    keys:\n        A sequence of strings representing the key of each table row to\n        fetch. String keys will be UTF-8 encoded.\n    require_all_keys:\n        If True only rows with values set for all keys will be returned.\n\n    Returns:\n    A dict mapping keys to the corresponding table row data\n    fetched. Each row is represented as a tuple of strings. For\n    example:\n\n    {b'Serak': ('Rigel VII', 'Preparer'),\n    b'Zim': ('Irk', 'Invader'),\n    b'Lrrr': ('Omicron Persei 8', 'Emperor')}\n\n    Returned keys are always bytes.  If a key from the keys argument is\n    missing from the dictionary, then that row was not found in the\n    table (and require_all_keys must have been False).\n\n    Raises:\n    IOError: An error occurred accessing the smalltable.\n    \"\"\"\n    return {}\n</code></pre>"}]}